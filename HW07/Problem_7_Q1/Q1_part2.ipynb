{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 7: Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.413837\n",
      "Epoch 2/15, Loss: 0.258707\n",
      "Epoch 3/15, Loss: 0.194295\n",
      "Epoch 4/15, Loss: 0.154911\n",
      "Epoch 5/15, Loss: 0.125964\n",
      "Epoch 6/15, Loss: 0.104109\n",
      "Epoch 7/15, Loss: 0.087502\n",
      "Epoch 8/15, Loss: 0.074660\n",
      "Epoch 9/15, Loss: 0.065063\n",
      "Epoch 10/15, Loss: 0.056267\n",
      "Epoch 11/15, Loss: 0.049808\n",
      "Epoch 12/15, Loss: 0.043685\n",
      "Epoch 13/15, Loss: 0.039220\n",
      "Epoch 14/15, Loss: 0.035655\n",
      "Epoch 15/15, Loss: 0.033154\n",
      "\n",
      "Accuracy: 97.8008%\n",
      "F1_Score: 80.3828%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.79      0.81      0.80     10398\n",
      "         dat       0.40      0.43      0.42       357\n",
      "       event       0.81      0.84      0.82       396\n",
      "         fac       0.79      0.85      0.82       281\n",
      "         loc       0.86      0.87      0.86      3238\n",
      "         mon       0.35      0.36      0.36       113\n",
      "         org       0.76      0.84      0.80      3941\n",
      "         pct       0.47      0.59      0.52        71\n",
      "         per       0.63      0.58      0.60       928\n",
      "        pers       0.94      0.95      0.95      1855\n",
      "         pro       0.83      0.95      0.88       419\n",
      "         tim       0.31      0.28      0.30        53\n",
      "\n",
      "   micro avg       0.79      0.82      0.80     22050\n",
      "   macro avg       0.66      0.70      0.68     22050\n",
      "weighted avg       0.79      0.82      0.80     22050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 128\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.400615\n",
      "Epoch 2/15, Loss: 0.240033\n",
      "Epoch 3/15, Loss: 0.178400\n",
      "Epoch 4/15, Loss: 0.138245\n",
      "Epoch 5/15, Loss: 0.111158\n",
      "Epoch 6/15, Loss: 0.090183\n",
      "Epoch 7/15, Loss: 0.074459\n",
      "Epoch 8/15, Loss: 0.063179\n",
      "Epoch 9/15, Loss: 0.054119\n",
      "Epoch 10/15, Loss: 0.046605\n",
      "Epoch 11/15, Loss: 0.041283\n",
      "Epoch 12/15, Loss: 0.036731\n",
      "Epoch 13/15, Loss: 0.032241\n",
      "Epoch 14/15, Loss: 0.029901\n",
      "Epoch 15/15, Loss: 0.025882\n",
      "\n",
      "Accuracy: 97.8385%\n",
      "F1_Score: 80.8847%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.77      0.82      0.79     10398\n",
      "         dat       0.43      0.57      0.49       357\n",
      "       event       0.81      0.91      0.86       396\n",
      "         fac       0.81      0.95      0.87       281\n",
      "         loc       0.84      0.89      0.86      3238\n",
      "         mon       0.38      0.62      0.47       113\n",
      "         org       0.78      0.84      0.81      3941\n",
      "         pct       0.43      0.68      0.53        71\n",
      "         per       0.61      0.70      0.65       928\n",
      "        pers       0.94      0.96      0.95      1855\n",
      "         pro       0.85      0.90      0.87       419\n",
      "         tim       0.37      0.43      0.40        53\n",
      "\n",
      "   micro avg       0.78      0.84      0.81     22050\n",
      "   macro avg       0.67      0.77      0.71     22050\n",
      "weighted avg       0.78      0.84      0.81     22050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 128\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 6\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Layers = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.385114\n",
      "Epoch 2/15, Loss: 0.228463\n",
      "Epoch 3/15, Loss: 0.166618\n",
      "Epoch 4/15, Loss: 0.128085\n",
      "Epoch 5/15, Loss: 0.100003\n",
      "Epoch 6/15, Loss: 0.080159\n",
      "Epoch 7/15, Loss: 0.064681\n",
      "Epoch 8/15, Loss: 0.054228\n",
      "Epoch 9/15, Loss: 0.045763\n",
      "Epoch 10/15, Loss: 0.039530\n",
      "Epoch 11/15, Loss: 0.033966\n",
      "Epoch 12/15, Loss: 0.030754\n",
      "Epoch 13/15, Loss: 0.027464\n",
      "Epoch 14/15, Loss: 0.024556\n",
      "Epoch 15/15, Loss: 0.022624\n",
      "\n",
      "Accuracy: 97.9356%\n",
      "F1_Score: 81.7282%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.79      0.81      0.80     10398\n",
      "         dat       0.47      0.49      0.48       357\n",
      "       event       0.82      0.92      0.87       396\n",
      "         fac       0.90      0.96      0.93       281\n",
      "         loc       0.85      0.90      0.87      3238\n",
      "         mon       0.32      0.39      0.35       113\n",
      "         org       0.80      0.83      0.81      3941\n",
      "         pct       0.45      0.63      0.53        71\n",
      "         per       0.67      0.66      0.66       928\n",
      "        pers       0.95      0.94      0.95      1855\n",
      "         pro       0.89      0.96      0.92       419\n",
      "         tim       0.30      0.42      0.35        53\n",
      "\n",
      "   micro avg       0.80      0.83      0.82     22050\n",
      "   macro avg       0.68      0.74      0.71     22050\n",
      "weighted avg       0.81      0.83      0.82     22050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 128\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 12\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Heads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.445432\n",
      "Epoch 2/15, Loss: 0.291151\n",
      "Epoch 3/15, Loss: 0.229309\n",
      "Epoch 4/15, Loss: 0.190191\n",
      "Epoch 5/15, Loss: 0.161963\n",
      "Epoch 6/15, Loss: 0.140347\n",
      "Epoch 7/15, Loss: 0.123273\n",
      "Epoch 8/15, Loss: 0.108695\n",
      "Epoch 9/15, Loss: 0.097365\n",
      "Epoch 10/15, Loss: 0.087595\n",
      "Epoch 11/15, Loss: 0.078884\n",
      "Epoch 12/15, Loss: 0.072299\n",
      "Epoch 13/15, Loss: 0.064373\n",
      "Epoch 14/15, Loss: 0.060194\n",
      "Epoch 15/15, Loss: 0.055486\n",
      "\n",
      "Accuracy: 97.3479%\n",
      "F1_Score: 75.8377%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.73      0.78      0.75     10398\n",
      "         dat       0.39      0.45      0.41       357\n",
      "       event       0.57      0.81      0.67       396\n",
      "         fac       0.72      0.90      0.80       281\n",
      "         loc       0.83      0.83      0.83      3238\n",
      "         mon       0.30      0.42      0.35       113\n",
      "         org       0.69      0.80      0.74      3941\n",
      "         pct       0.54      0.70      0.61        71\n",
      "         per       0.57      0.58      0.57       928\n",
      "        pers       0.87      0.94      0.90      1855\n",
      "         pro       0.74      0.86      0.80       419\n",
      "         tim       0.25      0.32      0.28        53\n",
      "\n",
      "   micro avg       0.73      0.79      0.76     22050\n",
      "   macro avg       0.60      0.70      0.64     22050\n",
      "weighted avg       0.73      0.79      0.76     22050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 128\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "ff_dim = 512\n",
    "num_layers = 2\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.443732\n",
      "Epoch 2/15, Loss: 0.287109\n",
      "Epoch 3/15, Loss: 0.225030\n",
      "Epoch 4/15, Loss: 0.185043\n",
      "Epoch 5/15, Loss: 0.155803\n",
      "Epoch 6/15, Loss: 0.132716\n",
      "Epoch 7/15, Loss: 0.115249\n",
      "Epoch 8/15, Loss: 0.100076\n",
      "Epoch 9/15, Loss: 0.087717\n",
      "Epoch 10/15, Loss: 0.077669\n",
      "Epoch 11/15, Loss: 0.069572\n",
      "Epoch 12/15, Loss: 0.062570\n",
      "Epoch 13/15, Loss: 0.056547\n",
      "Epoch 14/15, Loss: 0.050791\n",
      "Epoch 15/15, Loss: 0.046422\n",
      "\n",
      "Accuracy: 97.4674%\n",
      "F1_Score: 77.4394%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.74      0.79      0.76     10398\n",
      "         dat       0.35      0.46      0.40       357\n",
      "       event       0.72      0.84      0.77       396\n",
      "         fac       0.79      0.91      0.85       281\n",
      "         loc       0.82      0.87      0.85      3238\n",
      "         mon       0.21      0.24      0.22       113\n",
      "         org       0.72      0.81      0.76      3941\n",
      "         pct       0.34      0.42      0.38        71\n",
      "         per       0.59      0.62      0.61       928\n",
      "        pers       0.93      0.95      0.94      1855\n",
      "         pro       0.79      0.89      0.83       419\n",
      "         tim       0.19      0.26      0.22        53\n",
      "\n",
      "   micro avg       0.74      0.81      0.77     22050\n",
      "   macro avg       0.60      0.67      0.63     22050\n",
      "weighted avg       0.75      0.81      0.77     22050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 128\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 2\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Heads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.439810\n",
      "Epoch 2/15, Loss: 0.285379\n",
      "Epoch 3/15, Loss: 0.224124\n",
      "Epoch 4/15, Loss: 0.183414\n",
      "Epoch 5/15, Loss: 0.153741\n",
      "Epoch 6/15, Loss: 0.130982\n",
      "Epoch 7/15, Loss: 0.113381\n",
      "Epoch 8/15, Loss: 0.097937\n",
      "Epoch 9/15, Loss: 0.085868\n",
      "Epoch 10/15, Loss: 0.076247\n",
      "Epoch 11/15, Loss: 0.068403\n",
      "Epoch 12/15, Loss: 0.060848\n",
      "Epoch 13/15, Loss: 0.054310\n",
      "Epoch 14/15, Loss: 0.049642\n",
      "Epoch 15/15, Loss: 0.045174\n",
      "\n",
      "Accuracy: 97.2795%\n",
      "F1_Score: 75.7856%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.71      0.78      0.74     10398\n",
      "         dat       0.34      0.44      0.38       357\n",
      "       event       0.74      0.81      0.78       396\n",
      "         fac       0.86      0.89      0.87       281\n",
      "         loc       0.78      0.86      0.82      3238\n",
      "         mon       0.27      0.39      0.32       113\n",
      "         org       0.69      0.82      0.75      3941\n",
      "         pct       0.34      0.46      0.39        71\n",
      "         per       0.59      0.53      0.56       928\n",
      "        pers       0.92      0.94      0.93      1855\n",
      "         pro       0.81      0.87      0.84       419\n",
      "         tim       0.31      0.38      0.34        53\n",
      "\n",
      "   micro avg       0.72      0.80      0.76     22050\n",
      "   macro avg       0.61      0.68      0.64     22050\n",
      "weighted avg       0.72      0.80      0.76     22050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 128\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 16\n",
    "ff_dim = 512\n",
    "num_layers = 2\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Size: d_model = 64, ff_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.536494\n",
      "Epoch 2/15, Loss: 0.393015\n",
      "Epoch 3/15, Loss: 0.336848\n",
      "Epoch 4/15, Loss: 0.299713\n",
      "Epoch 5/15, Loss: 0.270997\n",
      "Epoch 6/15, Loss: 0.250266\n",
      "Epoch 7/15, Loss: 0.230171\n",
      "Epoch 8/15, Loss: 0.214150\n",
      "Epoch 9/15, Loss: 0.200083\n",
      "Epoch 10/15, Loss: 0.186691\n",
      "Epoch 11/15, Loss: 0.176203\n",
      "Epoch 12/15, Loss: 0.165114\n",
      "Epoch 13/15, Loss: 0.155822\n",
      "Epoch 14/15, Loss: 0.147902\n",
      "Epoch 15/15, Loss: 0.139299\n",
      "\n",
      "Accuracy: 95.0960%\n",
      "F1_Score: 59.5711%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.58      0.63      0.61     10398\n",
      "         dat       0.32      0.26      0.28       357\n",
      "       event       0.23      0.38      0.29       396\n",
      "         fac       0.31      0.40      0.35       281\n",
      "         loc       0.67      0.74      0.71      3238\n",
      "         mon       0.10      0.12      0.11       113\n",
      "         org       0.50      0.60      0.55      3941\n",
      "         pct       0.17      0.21      0.19        71\n",
      "         per       0.49      0.46      0.48       928\n",
      "        pers       0.73      0.81      0.77      1855\n",
      "         pro       0.43      0.42      0.42       419\n",
      "         tim       0.02      0.02      0.02        53\n",
      "\n",
      "   micro avg       0.57      0.63      0.60     22050\n",
      "   macro avg       0.38      0.42      0.40     22050\n",
      "weighted avg       0.57      0.63      0.60     22050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 128\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "num_layers = 2\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Size: d_model = 192, ff_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.390419\n",
      "Epoch 2/15, Loss: 0.224583\n",
      "Epoch 3/15, Loss: 0.162898\n",
      "Epoch 4/15, Loss: 0.125233\n",
      "Epoch 5/15, Loss: 0.098022\n",
      "Epoch 6/15, Loss: 0.079726\n",
      "Epoch 7/15, Loss: 0.065648\n",
      "Epoch 8/15, Loss: 0.055045\n",
      "Epoch 9/15, Loss: 0.047043\n",
      "Epoch 10/15, Loss: 0.040298\n",
      "Epoch 11/15, Loss: 0.035810\n",
      "Epoch 12/15, Loss: 0.031402\n",
      "Epoch 13/15, Loss: 0.028190\n",
      "Epoch 14/15, Loss: 0.025423\n",
      "Epoch 15/15, Loss: 0.023305\n",
      "\n",
      "Accuracy: 97.9275%\n",
      "F1_Score: 81.1314%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.80      0.81      0.80     10398\n",
      "         dat       0.40      0.46      0.43       357\n",
      "       event       0.88      0.89      0.89       396\n",
      "         fac       0.87      0.94      0.90       281\n",
      "         loc       0.86      0.89      0.87      3238\n",
      "         mon       0.34      0.40      0.36       113\n",
      "         org       0.80      0.81      0.80      3941\n",
      "         pct       0.51      0.58      0.54        71\n",
      "         per       0.63      0.61      0.62       928\n",
      "        pers       0.92      0.96      0.94      1855\n",
      "         pro       0.88      0.90      0.89       419\n",
      "         tim       0.32      0.38      0.35        53\n",
      "\n",
      "   micro avg       0.80      0.82      0.81     22050\n",
      "   macro avg       0.68      0.72      0.70     22050\n",
      "weighted avg       0.80      0.82      0.81     22050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 128\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 192\n",
    "num_heads = 4\n",
    "ff_dim = 768\n",
    "num_layers = 2\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Size: d_model = 256, ff_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.351152\n",
      "Epoch 2/15, Loss: 0.185150\n",
      "Epoch 3/15, Loss: 0.124825\n",
      "Epoch 4/15, Loss: 0.090603\n",
      "Epoch 5/15, Loss: 0.067479\n",
      "Epoch 6/15, Loss: 0.052440\n",
      "Epoch 7/15, Loss: 0.042068\n",
      "Epoch 8/15, Loss: 0.035619\n",
      "Epoch 9/15, Loss: 0.029857\n",
      "Epoch 10/15, Loss: 0.024965\n",
      "Epoch 11/15, Loss: 0.022335\n",
      "Epoch 12/15, Loss: 0.020423\n",
      "Epoch 13/15, Loss: 0.018143\n",
      "Epoch 14/15, Loss: 0.016360\n",
      "Epoch 15/15, Loss: 0.015407\n",
      "\n",
      "Accuracy: 98.1125%\n",
      "F1_Score: 82.3825%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.80      0.82      0.81     10398\n",
      "         dat       0.45      0.49      0.47       357\n",
      "       event       0.85      0.92      0.88       396\n",
      "         fac       0.95      0.97      0.96       281\n",
      "         loc       0.87      0.89      0.88      3238\n",
      "         mon       0.37      0.41      0.38       113\n",
      "         org       0.81      0.85      0.83      3941\n",
      "         pct       0.49      0.61      0.54        71\n",
      "         per       0.64      0.58      0.61       928\n",
      "        pers       0.93      0.96      0.95      1855\n",
      "         pro       0.91      0.94      0.92       419\n",
      "         tim       0.48      0.62      0.54        53\n",
      "\n",
      "   micro avg       0.81      0.83      0.82     22050\n",
      "   macro avg       0.71      0.75      0.73     22050\n",
      "weighted avg       0.81      0.83      0.82     22050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 128\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 256\n",
    "num_heads = 4\n",
    "ff_dim = 1024\n",
    "num_layers = 2\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Sequence Length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.443980\n",
      "Epoch 2/15, Loss: 0.287881\n",
      "Epoch 3/15, Loss: 0.224035\n",
      "Epoch 4/15, Loss: 0.184461\n",
      "Epoch 5/15, Loss: 0.155134\n",
      "Epoch 6/15, Loss: 0.133605\n",
      "Epoch 7/15, Loss: 0.115807\n",
      "Epoch 8/15, Loss: 0.101513\n",
      "Epoch 9/15, Loss: 0.089456\n",
      "Epoch 10/15, Loss: 0.079369\n",
      "Epoch 11/15, Loss: 0.070601\n",
      "Epoch 12/15, Loss: 0.063626\n",
      "Epoch 13/15, Loss: 0.057222\n",
      "Epoch 14/15, Loss: 0.051651\n",
      "Epoch 15/15, Loss: 0.047680\n",
      "\n",
      "Accuracy: 97.5462%\n",
      "F1_Score: 78.0052%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.76      0.80      0.78     10158\n",
      "         dat       0.36      0.45      0.40       356\n",
      "       event       0.75      0.81      0.78       390\n",
      "         fac       0.74      0.81      0.78       269\n",
      "         loc       0.79      0.87      0.83      3184\n",
      "         mon       0.30      0.38      0.33        96\n",
      "         org       0.74      0.80      0.77      3875\n",
      "         pct       0.45      0.55      0.49        71\n",
      "         per       0.61      0.58      0.60       910\n",
      "        pers       0.91      0.97      0.94      1795\n",
      "         pro       0.84      0.89      0.87       402\n",
      "         tim       0.28      0.34      0.31        53\n",
      "\n",
      "   micro avg       0.75      0.81      0.78     21559\n",
      "   macro avg       0.63      0.69      0.66     21559\n",
      "weighted avg       0.76      0.81      0.78     21559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 64\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 2\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Sequence Length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.438014\n",
      "Epoch 2/15, Loss: 0.288725\n",
      "Epoch 3/15, Loss: 0.227379\n",
      "Epoch 4/15, Loss: 0.187588\n",
      "Epoch 5/15, Loss: 0.158676\n",
      "Epoch 6/15, Loss: 0.137224\n",
      "Epoch 7/15, Loss: 0.119525\n",
      "Epoch 8/15, Loss: 0.103801\n",
      "Epoch 9/15, Loss: 0.092007\n",
      "Epoch 10/15, Loss: 0.082012\n",
      "Epoch 11/15, Loss: 0.073142\n",
      "Epoch 12/15, Loss: 0.066046\n",
      "Epoch 13/15, Loss: 0.059298\n",
      "Epoch 14/15, Loss: 0.054300\n",
      "Epoch 15/15, Loss: 0.049690\n",
      "\n",
      "Accuracy: 97.4764%\n",
      "F1_Score: 77.8033%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.75      0.79      0.77     10449\n",
      "         dat       0.35      0.50      0.41       357\n",
      "       event       0.73      0.80      0.76       396\n",
      "         fac       0.79      0.85      0.82       281\n",
      "         loc       0.82      0.87      0.84      3240\n",
      "         mon       0.28      0.43      0.34       114\n",
      "         org       0.75      0.80      0.77      3945\n",
      "         pct       0.44      0.62      0.51        71\n",
      "         per       0.57      0.64      0.60       938\n",
      "        pers       0.90      0.92      0.91      1874\n",
      "         pro       0.72      0.83      0.77       435\n",
      "         tim       0.28      0.40      0.33        53\n",
      "\n",
      "   micro avg       0.75      0.80      0.78     22153\n",
      "   macro avg       0.61      0.70      0.65     22153\n",
      "weighted avg       0.75      0.80      0.78     22153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 256\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 2\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Sequence Length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.440509\n",
      "Epoch 2/15, Loss: 0.286984\n",
      "Epoch 3/15, Loss: 0.224916\n",
      "Epoch 4/15, Loss: 0.186276\n",
      "Epoch 5/15, Loss: 0.158530\n",
      "Epoch 6/15, Loss: 0.135651\n",
      "Epoch 7/15, Loss: 0.118433\n",
      "Epoch 8/15, Loss: 0.104799\n",
      "Epoch 9/15, Loss: 0.092508\n",
      "Epoch 10/15, Loss: 0.082337\n",
      "Epoch 11/15, Loss: 0.073490\n",
      "Epoch 12/15, Loss: 0.066266\n",
      "Epoch 13/15, Loss: 0.060150\n",
      "Epoch 14/15, Loss: 0.054817\n",
      "Epoch 15/15, Loss: 0.050072\n",
      "\n",
      "Accuracy: 97.3677%\n",
      "F1_Score: 77.0357%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.75      0.79      0.77     10449\n",
      "         dat       0.36      0.50      0.42       357\n",
      "       event       0.69      0.81      0.74       396\n",
      "         fac       0.72      0.91      0.80       281\n",
      "         loc       0.81      0.87      0.84      3240\n",
      "         mon       0.31      0.42      0.36       114\n",
      "         org       0.73      0.76      0.75      3945\n",
      "         pct       0.43      0.58      0.49        71\n",
      "         per       0.58      0.61      0.59       938\n",
      "        pers       0.89      0.94      0.91      1874\n",
      "         pro       0.72      0.84      0.77       435\n",
      "         tim       0.24      0.40      0.30        53\n",
      "\n",
      "   micro avg       0.74      0.80      0.77     22153\n",
      "   macro avg       0.60      0.70      0.65     22153\n",
      "weighted avg       0.75      0.80      0.77     22153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 512\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 2\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Transformer NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.322535\n",
      "Epoch 2/15, Loss: 0.155475\n",
      "Epoch 3/15, Loss: 0.097106\n",
      "Epoch 4/15, Loss: 0.065427\n",
      "Epoch 5/15, Loss: 0.047040\n",
      "Epoch 6/15, Loss: 0.036565\n",
      "Epoch 7/15, Loss: 0.028971\n",
      "Epoch 8/15, Loss: 0.025113\n",
      "Epoch 9/15, Loss: 0.020459\n",
      "Epoch 10/15, Loss: 0.019079\n",
      "Epoch 11/15, Loss: 0.017134\n",
      "Epoch 12/15, Loss: 0.014896\n",
      "Epoch 13/15, Loss: 0.013966\n",
      "Epoch 14/15, Loss: 0.013130\n",
      "Epoch 15/15, Loss: 0.012225\n",
      "\n",
      "Accuracy: 98.2762%\n",
      "F1_Score: 84.8302%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.82      0.84      0.83     10158\n",
      "         dat       0.47      0.41      0.44       356\n",
      "       event       0.95      0.96      0.96       390\n",
      "         fac       0.93      0.97      0.95       269\n",
      "         loc       0.88      0.91      0.89      3184\n",
      "         mon       0.39      0.50      0.44        96\n",
      "         org       0.83      0.89      0.86      3875\n",
      "         pct       0.53      0.77      0.63        71\n",
      "         per       0.71      0.66      0.68       910\n",
      "        pers       0.96      0.97      0.97      1795\n",
      "         pro       0.97      0.98      0.98       402\n",
      "         tim       0.48      0.64      0.55        53\n",
      "\n",
      "   micro avg       0.84      0.86      0.85     21559\n",
      "   macro avg       0.74      0.79      0.76     21559\n",
      "weighted avg       0.84      0.86      0.85     21559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_clean_ner_data(tokens_path, labels_path):\n",
    "    with open(tokens_path, \"r\", encoding=\"utf-8\") as token_file, open(labels_path, \"r\", encoding=\"utf-8\") as label_file:\n",
    "        token_lines = token_file.readlines()\n",
    "        label_lines = label_file.readlines()\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    \n",
    "    for t_line, l_line in zip(token_lines, label_lines):\n",
    "        tokens = t_line.strip().split()\n",
    "        lbls = l_line.strip().split()\n",
    "        if len(tokens) == len(lbls) and len(tokens) > 0:\n",
    "            sentences.append([t.strip() for t in tokens])\n",
    "            labels.append([l.strip().lower().replace(\"_\", \"-\") for l in lbls])\n",
    "    return sentences, labels\n",
    "\n",
    "# Read and merge ARMAN & PEYMA datasets\n",
    "arman_sent, arman_lab = load_clean_ner_data(\"arman-tokens.txt\", \"arman-labels.txt\")\n",
    "peyma_sent, peyma_lab = load_clean_ner_data(\"peyma-tokens.txt\", \"peyma-labels.txt\")\n",
    "all_sentences = arman_sent + peyma_sent\n",
    "all_labels = arman_lab + peyma_lab\n",
    "\n",
    "# Combine and shuffle all data, then split into train (%80) and test (%20) sets\n",
    "combined = list(zip(all_sentences, all_labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "train_sentences, train_labels = zip(*combined[:split_idx])\n",
    "test_sentences, test_labels = zip(*combined[split_idx:])\n",
    "\n",
    "all_labels_unique = sorted(set(l for seq in (train_labels + test_labels) for l in seq))\n",
    "label2id = {l: i for i, l in enumerate(all_labels_unique)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "token_counter = Counter(chain(*train_sentences))\n",
    "token2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tok, count in token_counter.items():\n",
    "    if count > 1:\n",
    "        token2id[tok] = len(token2id)\n",
    "id2token = {i: t for t, i in token2id.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NER dataset with padding and masks\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, max_len, token2id, label2id):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [self.token2id.get(t, self.token2id[\"<unk>\"]) for t in self.sentences[idx]]\n",
    "        tags = [self.label2id[l] for l in self.labels[idx]]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tags = tags[:self.max_len]\n",
    "        attn_mask = [1] * len(tokens)\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens += [0] * pad_len\n",
    "        tags += [-100] * pad_len\n",
    "        attn_mask += [0] * pad_len\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens),\n",
    "            \"labels\": torch.tensor(tags),\n",
    "            \"attention_mask\": torch.tensor(attn_mask)\n",
    "        }\n",
    "\n",
    "# Prepare NER datasets and loaders\n",
    "MAX_LEN = 64\n",
    "train_data = NERDataset(train_sentences, train_labels, MAX_LEN, token2id, label2id)\n",
    "test_data = NERDataset(test_sentences, test_labels, MAX_LEN, token2id, label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding for token positions\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Standard multi-head self-attention\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B = q.size(0)\n",
    "        q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = attn @ v\n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # Feed-forward block used in transformer\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    # Wraps sublayer with norm, dropout, and residual\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.ffn)\n",
    "\n",
    "# Full transformer-based NER model\n",
    "class TransformerNER(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers, max_len, num_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, 0.1) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, mask, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 256\n",
    "num_heads = 4\n",
    "ff_dim = 1024\n",
    "num_layers = 6\n",
    "vocab_size = len(token2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = TransformerNER(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    heads=num_heads,\n",
    "    d_ff=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_len=MAX_LEN,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/15, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Predict on test set \n",
    "model.eval()\n",
    "all_preds,all_labels_eval = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids, mask)[\"logits\"].argmax(-1)\n",
    "\n",
    "        for p, l, m in zip(logits, labels, mask):\n",
    "            true_seq = [id2label[i.item()] for i, msk in zip(l, m) if i.item() != -100 and msk.item() == 1]\n",
    "            pred_seq = [id2label[i.item()] for i, msk, gt in zip(p, m, l) if gt.item() != -100 and msk.item() == 1]\n",
    "            all_labels_eval.append(true_seq)\n",
    "            all_preds.append(pred_seq)\n",
    "\n",
    "# compute evaluation metrics\n",
    "accuracy = 100 * accuracy_score(all_labels_eval, all_preds)\n",
    "f1 = 100 * f1_score(all_labels_eval, all_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}%\")\n",
    "print(f\"F1_Score: {f1:.4f}%\")\n",
    "print(classification_report(all_labels_eval, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
